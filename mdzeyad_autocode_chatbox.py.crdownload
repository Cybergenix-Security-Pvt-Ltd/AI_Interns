# -*- coding: utf-8 -*-
"""Untitled49.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nn1VzhjtLHi-hCWp4JbXRqV3gdqzUvS9
"""

!pip install transformers
!pip install accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model & tokenizer
model_name = "Salesforce/codegen-350M-mono"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Step 3: Get Prompt and Generate Code (Fixed with attention_mask and pad_token_id)

# Get the prompt from the user
user_prompt = input("ðŸ’¬ Enter a coding task (e.g., Python code to bubble sort :\n")

# Format prompt
formatted_prompt = f"# {user_prompt}\n"

# Encode input prompt with attention mask
inputs = tokenizer.encode_plus(
    formatted_prompt,
    return_tensors="pt",
    return_attention_mask=True
)

input_ids = inputs["input_ids"].to(device)
attention_mask = inputs["attention_mask"].to(device)

# Generate output with attention mask and pad token id
outputs = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=256,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    pad_token_id=tokenizer.eos_token_id
)

# Decode generated tokens
generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Remove the input part to show only generated code
clean_code = generated_code.replace(formatted_prompt, "").strip()

print("âœ… Generated Code:\n")
print(clean_code)

# Save the code to a file
file_name = "autocode_output.py"  # change extension if needed

with open(file_name, "w", encoding="utf-8") as f:
    f.write(clean_code)

print(f"ðŸ’¾ Code saved to {file_name}")

# Download file in Colab
from google.colab import files
files.download(file_name)